{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c946b9af",
      "metadata": {},
      "source": [
        "# Exercise: GenAI Evaluation Medley\n",
        "\n",
        "Let's practice exact evaluation, AI-as-judge mechanics, and benchmarking by completing small, focused coding tasks.\n",
        "\n",
        "## Outline\n",
        "\n",
        "We will cover the following evaluation techniques:\n",
        "1. Exact Match: Implement a function to compute the exact match score between predicted and reference answers.\n",
        "2. Lexical Similarity: Calculate ROUGE scores to assess the overlap between predicted and reference texts.\n",
        "3. Semantic Similarity: Use embeddings to compute cosine similarity between predicted and reference texts.\n",
        "4. Functional Correctness: Evaluate code generation by executing predicted code and comparing outputs.\n",
        "5. Pass@K: Implement the Pass@K metric.\n",
        "6. LLM-as-a-Judge or AI-as-a-Judge: Use a language model to evaluate the quality of predictions based on a rubric."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72f89e6",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Now we import standard libraries used across exercises and set basic configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b2778307",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Vocareum OpenAI API base URL\n"
          ]
        }
      ],
      "source": [
        "# Student Task: Set up the OpenAI API key and base URL from environment variables\n",
        "# TODO: If using Vocareum, set the API key directly in the code below\n",
        "\n",
        "import litellm\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "if os.getenv(\"OPENAI_API_KEY\"):\n",
        "    litellm.openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# If using Vocareum, you can also set your API key here directly\n",
        "# Uncomment and replace the string with your actual Vocareum API key\n",
        "litellm.openai_key = \"voc-227759408182772483436569978bec52c6a1.60684630\"\n",
        "\n",
        "if (litellm.openai_key or \"\").startswith(\"voc-\"):\n",
        "    litellm.api_base = \"https://openai.vocareum.com/v1\"\n",
        "    print(\"Using Vocareum OpenAI API base URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cc74fd4",
      "metadata": {},
      "source": [
        "## Exact Match (EM)\n",
        "Let's compute exact-match accuracy after simple normalization (lowercase and trim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58b84285",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM: 0.75\n"
          ]
        }
      ],
      "source": [
        "# Student task: Implement exact_match and compute EM\n",
        "# TODO: Complete the sections marked with **********\n",
        "\n",
        "preds = [\"Lima\", \"ayacucho\", \"Cusco\", \"Arequipa\"]\n",
        "labels = [\"lima\", \"Ayacucho\", \"Cusco\", \"Trujillo\"]\n",
        "\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    \"\"\"Normalize the string by lowercasing and stripping whitespace.\"\"\"\n",
        "    return s.lower().strip()\n",
        "\n",
        "\n",
        "def exact_match(pred: str, label: str) -> int:\n",
        "    # return 1 if normalized strings are identical, else 0\n",
        "    return_value = 1 if normalize(pred) == normalize(label) else 0\n",
        "\n",
        "\n",
        "    return return_value\n",
        "\n",
        "\n",
        "em_scores = [exact_match(p, l) for p, l in zip(preds, labels)]\n",
        "em = sum(em_scores) / len(em_scores)\n",
        "print(\"EM:\", em)\n",
        "\n",
        "assert em == 0.75, (\n",
        "    f\"EM should be 0.75, but got {em}. Please check your exact_match function.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c71e252",
      "metadata": {},
      "source": [
        "## Lexical Similarity (ROUGE)\n",
        "\n",
        "Let's compute ROUGE scores using the `evaluate` library.\n",
        "\n",
        "Read more at: https://huggingface.co/spaces/evaluate-metric/rouge/blob/main/README.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b039506b",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Leon\\miniconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': 1.0,\n",
              " 'rouge2': 0.6,\n",
              " 'rougeL': 0.6666666666666666,\n",
              " 'rougeLsum': 0.6666666666666666}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Student task: Compute ROUGE-L using LCS length\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "# Define candidate and reference texts\n",
        "import evaluate\n",
        "\n",
        "\n",
        "pred = \"The capital of Peru is Lima\"\n",
        "label = \"Lima is the capital of Peru\"\n",
        "\n",
        "\n",
        "# Import the evaluate library\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "# Load the ROUGE metric\n",
        "results = rouge.compute(predictions=[pred], references=[label])\n",
        "\n",
        "# Compute ROUGE scores\n",
        "# **************\n",
        "\n",
        "\n",
        "assert isinstance(results, dict), (\n",
        "    f\"Results should be a dictionary, but got {type(results)}. See the evaluate library documentation for ROUGE usage.\"\n",
        ")\n",
        "keys = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "for key in keys:\n",
        "    assert key in results, (\n",
        "        f\"Missing key '{key}' in results. Expected keys: {keys}. See the evaluate library documentation for ROUGE usage.\"\n",
        "    )\n",
        "\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15822ca6",
      "metadata": {},
      "source": [
        "## Semantic Similarity using Cosine Similarity\n",
        "\n",
        "We'll use the `sentence-transformers` library to compute semantic similarity between predicted and reference sentences. The model \"all-MiniLM-L6-v2\" is a lightweight model that can run on GPUs.\n",
        "\n",
        "Read more here: https://sbert.net/docs/quickstart.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5d276d34",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Load a pretrained Sentence Transformer model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. Some example sentences\n",
        "sentences = [\n",
        "    \"Hi there!\",\n",
        "    \"This is a test sentence.\",\n",
        "]\n",
        "\n",
        "# 3. Generate embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Verify we have 2 embeddings of dimension 384 each\n",
        "assert embeddings.shape == (2, 384)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9a78dfbf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[ 0.06145659, -0.06237292, -0.03735719, ..., -0.00960596,\n",
              "          0.03519067, -0.0140261 ],\n",
              "        [-0.01852841, -0.03180065, -0.07411853, ..., -0.00197015,\n",
              "          0.01199757,  0.0113026 ],\n",
              "        [-0.01629126,  0.1040661 ,  0.09740778, ...,  0.00676725,\n",
              "         -0.08788463,  0.03404381]], dtype=float32),\n",
              " array([[ 0.06701851, -0.0406396 , -0.06178873, ...,  0.01089181,\n",
              "         -0.01366578, -0.02568763],\n",
              "        [ 0.08464755,  0.00272664, -0.06455816, ...,  0.04696646,\n",
              "         -0.06039638, -0.00335862],\n",
              "        [ 0.03886198, -0.02831237, -0.02234175, ...,  0.00904633,\n",
              "         -0.02847457, -0.00952085]], dtype=float32))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Student task: Write a semantically different prediction sentence and compute embeddings\n",
        "# Complete the sections with **********\n",
        "\n",
        "labels = [\"Cusco is in Peru\", \"Ayacucho is a region\", \"Trujillo beaches are marvelous\"]\n",
        "preds = [\n",
        "    \"Peru includes Cusco\",\n",
        "    \"Ayacucho is a department\",\n",
        "    # Write a sentence that is very semantically different from the prediction\n",
        "    \"Today is a sunny day\"\n",
        "    ]\n",
        "\n",
        "\n",
        "# Get the embeddings for each sentence\n",
        "pred_embeddings = model.encode(preds)\n",
        "label_embeddings = model.encode(labels)\n",
        "\n",
        "assert pred_embeddings.shape == (3, 384), (\n",
        "    f\"Expected shape (3, 384), got {pred_embeddings.shape}\"\n",
        ")\n",
        "assert label_embeddings.shape == (3, 384), (\n",
        "    f\"Expected shape (3, 384), got {label_embeddings.shape}\"\n",
        ")\n",
        "\n",
        "pred_embeddings, label_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6e3b346c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pair 1:\n",
            "  Pred: Peru includes Cusco\n",
            "  Label: Cusco is in Peru\n",
            "  Cosine Similarity: 0.9358\n",
            "\n",
            "Pair 2:\n",
            "  Pred: Ayacucho is a department\n",
            "  Label: Ayacucho is a region\n",
            "  Cosine Similarity: 0.7663\n",
            "\n",
            "Pair 3:\n",
            "  Pred: Today is a sunny day\n",
            "  Label: Trujillo beaches are marvelous\n",
            "  Cosine Similarity: 0.1020\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the cosine similarity for each pair of embeddings\n",
        "# No changes needed in this cell, but if it fails, check the above cell\n",
        "\n",
        "cosine_similarity = [\n",
        "    # Cosine similarity for two vectors a and b is defined as:\n",
        "    # cos_sim(a, b) = (a . b) / (||a|| * ||b||)\n",
        "    # where (a . b) is the dot product of a and b,\n",
        "    # and ||a|| and ||b|| are the magnitudes (norms) of vectors a and b respectively.\n",
        "    float(\n",
        "        np.dot(pred_embeddings[i], label_embeddings[i])\n",
        "        / np.linalg.norm(pred_embeddings[i])\n",
        "        / np.linalg.norm(label_embeddings[i])\n",
        "    )\n",
        "    for i in range(len(preds))\n",
        "]\n",
        "\n",
        "# Compute cosine similarity between the two embeddings\n",
        "for i, (p, l, cos_sim) in enumerate(zip(preds, labels, cosine_similarity)):\n",
        "    print(f\"Pair {i + 1}:\")\n",
        "    print(f\"  Pred: {p}\")\n",
        "    print(f\"  Label: {l}\")\n",
        "    print(f\"  Cosine Similarity: {cos_sim:.4f}\\n\")\n",
        "\n",
        "# Check that the last pair has the lowest similarity\n",
        "assert cosine_similarity[-1] < cosine_similarity[0], (\n",
        "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
        ")\n",
        "assert cosine_similarity[-1] < cosine_similarity[1], (\n",
        "    \"The last pair should have the lowest cosine similarity. Please check your prediction sentence.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbaf675",
      "metadata": {},
      "source": [
        "## Functional Correctness\n",
        "Let's evaluate code-generation by running a tiny function against unit tests (execution accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6ecc358d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proportion of tests passed: 0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "# Student task: Complete the evaluation of the sort_and_normalize function\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "def sort_and_normalize(s: str) -> str:\n",
        "    \"\"\"Sort the words in the string\"\"\"\n",
        "\n",
        "    # Our toy function will fail on this edge case\n",
        "    if \"armadillo\" in s:\n",
        "        s = s.replace(\"armadillo\", \"kitty\")\n",
        "\n",
        "    return \" \".join(sorted(s.split()))\n",
        "\n",
        "\n",
        "preds = [\n",
        "    \"the capybara is the largest rodent\",\n",
        "    \"an armadillo has a hard shell\",\n",
        "    \"elephants are the largest land animals\",\n",
        "]\n",
        "labels = [\n",
        "    \"capybara is largest rodent the the\",\n",
        "    \"a an armadillo hard has shell\",\n",
        "    \"animals are elephants land largest the\",\n",
        "]\n",
        "\n",
        "# Write tests to check if sort_and_normalize works correctly\n",
        "results = [\n",
        "    sort_and_normalize(p) == l\n",
        "    for p, l in zip(preds, labels)\n",
        "]\n",
        "\n",
        "print(\"Proportion of tests passed:\", sum(results) / len(results))\n",
        "\n",
        "assert sum(results) == 2, (\n",
        "    f\"2 tests should pass, but got {sum(results)}. Please check how your are evaluating the results.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5b5ab6",
      "metadata": {},
      "source": [
        "## Pass@k\n",
        "\n",
        "Let's simulate multiple samples for a single task and compute pass@k (1 if any sample equals the gold)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5459f06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pass@4 = 1\n"
          ]
        }
      ],
      "source": [
        "# Student task: Implement pass_at_k\n",
        "# Complete the sections with **********\n",
        "\n",
        "label = \"Lima\"\n",
        "samples = [\"Lima\", \"Arequipa\", \"Cusco\", \"Lima\"]\n",
        "\n",
        "\n",
        "# Implement pass_at_k with signature (samples: List[str], label: str) -> int\n",
        "def pass_at_k(samples, label):\n",
        "    return int(any(s == label for s in samples))\n",
        "\n",
        "\n",
        "\n",
        "print(\"pass@4 =\", pass_at_k(samples, label))\n",
        "\n",
        "assert pass_at_k(samples, label) == 1, (\n",
        "    f\"pass@4 should be 1, but got {pass_at_k(samples, label)}. Please check your pass_at_k function.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b336e1",
      "metadata": {},
      "source": [
        "## LLM as a Judge\n",
        "\n",
        "Let's create a function that calls an LLM to compare predicted values and reference values (if applicable) and return a score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "97697d6b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM response: <reasoning>Manila is the capital city of the Philippines, which exactly matches the label. Therefore, the prediction is correct and earns the maximum score.</reasoning>\n",
            "<score>1.0</score>\n",
            "LLM response: <reasoning>Cebu is a city located in the Philippines. The capital of the Philippines is Manila. Since the prediction is a city within the same country but not the capital, the score is 0.5.</reasoning>\n",
            "<score>0.5</score>\n",
            "LLM response: <reasoning>Tokyo is not in the Philippines and is not the capital of the Philippines; Manila is the capital. Therefore the prediction does not match the country.</reasoning>\n",
            "<score>0.0</score>\n"
          ]
        }
      ],
      "source": [
        "# Student task: Complete the LLM-as-a-judge function\n",
        "# Complete the sections with **********\n",
        "\n",
        "\n",
        "def llm_as_judge(pred, rubric, label ):\n",
        "    \"\"\"Use an LLM to judge the quality of a prediction against a rubric and optional label.\"\"\"\n",
        "    from litellm import completion\n",
        "\n",
        "    # Write a system prompt that instructs the LLM to use the rubric to score the prediction\n",
        "    # The response should be formatted as:\n",
        "    # <reasoning>...</reasoning>\n",
        "    # <score>FLOAT_ANSWER</score>\n",
        "    # where FLOAT_ANSWER is a float between 0 and 1.\n",
        "    # We will extract FLOAT_ANSWER from the response later\n",
        "\n",
        "    SYSTEM_PROMPT = f\"\"\"You are an expert evaluator use the following rubric to score the prediction.\n",
        "    The response should be formatted as:\n",
        "    <reasoning>...</reasoning>\n",
        "    <score>FLOAT_ANSWER</score>\n",
        "    where FLOAT_ANSWER is a float between 0 and 1.\n",
        "    RUBRIC:\n",
        "    {rubric}\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a user prompt with the prediction and, optionally, the label\n",
        "    USER_PROMPT = f\"Prediction: {pred}\\n\"\n",
        "    if label is not None:\n",
        "        USER_PROMPT += f\"Label: {label}\\n\"\n",
        "\n",
        "\n",
        "    # Call the LLM using litellm with the system and user prompts (use the model gpt-5-nano)\n",
        "    # See: https://github.com/BerriAI/litellm\n",
        "\n",
        "    response = completion(\n",
        "        model=\"gpt-5-nano\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": USER_PROMPT},\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "    text_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(\"LLM response:\", text_response)\n",
        "\n",
        "    # Extract FLOAT_ANSWER from the response\n",
        "\n",
        "    float_answer = float(text_response.split(\"<score>\")[-1].split(\"</score>\")[0].strip())\n",
        "\n",
        "\n",
        "    return float_answer\n",
        "\n",
        "\n",
        "# Write a rubric for evaluating if the prediction is the capital of the label country\n",
        "# 1.0 if correct, 0.5 if a city in the same country, 0.0 otherwise\n",
        "\n",
        "RUBRIC = \"\"\"Score the prediction based on the following criteria:\n",
        "- If the prediction is the capital city of the country specified in the label, score 1.0\n",
        "- If the prediction is a city in the same country as the label but not the capital, score 0.5\n",
        "- If the prediction is not a city in the same country as the label, score 0.0\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Manila\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 1.0\n",
        "), \"Manila is the capital of the Philippines\"\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Cebu\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 0.5\n",
        "), \"Cebu is a city in the Philippines, but not the capital\"\n",
        "\n",
        "assert (\n",
        "    llm_as_judge(\n",
        "        pred=\"Tokyo\",\n",
        "        label=\"Philippines\",\n",
        "        rubric=RUBRIC,\n",
        "    )\n",
        "    == 0.0\n",
        "), \"Tokyo is not in the Philippines\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa792f32",
      "metadata": {},
      "source": [
        "Congrats! You have completed the evaluation exercise. Proper evaluation is the bedrock for building reliable AI systems. Great job! üëèüëèüëè"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4089a86",
      "metadata": {},
      "source": [
        "<br /><br /><br /><br /><br /><br /><br /><br /><br />"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
